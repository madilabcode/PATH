{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQ_dTka-Thaa"
      },
      "source": [
        "\n",
        "# PATH Model Tutorial\n",
        "\n",
        "This notebook demonstrates the complete workflow for using the PATH model (TransPath) for spatial transcriptomics analysis. \n",
        "\n",
        "## Overview\n",
        "\n",
        "The PATH (TransPath) model is designed to analyze spatial transcriptomics data by extracting meaningful embeddings from tissue images and predicting pathway activities. This tutorial walks through:\n",
        "\n",
        "1. **Environment Setup**: Installing required libraries and cloning the PATH repository\n",
        "2. **Data & Model Retrieval**: Downloading pre-trained model weights and example datasets\n",
        "3. **Data Processing**: Loading and preprocessing spatial transcriptomics data (.h5ad format)\n",
        "4. **Model Inference**: Initializing the model, loading weights, and extracting embeddings\n",
        "5. **Downstream Analysis**: Clustering, pathway prediction, and visualization\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Python environment with CUDA support (recommended for GPU acceleration)\n",
        "- Sufficient disk space for model weights and datasets (~several GB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Install Dependencies\n",
        "\n",
        "Install the required Python packages:\n",
        "- `scanpy`: For handling spatial transcriptomics data\n",
        "- `gdown`: For downloading files from Google Drive\n",
        "- `igraph`: For graph-based analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSZ7xEqLD1TQ"
      },
      "outputs": [],
      "source": [
        "!pip install scanpy gdown\n",
        "!git clone https://github.com/madilabcode/PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Clone TransPath Repository\n",
        "\n",
        "Change to the PATH directory and run the setup script to clone the TransPath repository. This script will set up the necessary dependencies and directory structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-l7th3qEeY-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir(\"PATH\")\n",
        "!chmod 777  ./clone_transpath.sh\n",
        "!./clone_transpath.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Download Data and Model Weights\n",
        "\n",
        "Download the required files from Google Drive:\n",
        "- **hd_obj.h5ad**: Example spatial transcriptomics dataset (AnnData format)\n",
        "- **hd_wights.pth**: Pre-trained model weights for the PATH model\n",
        "- **mask_hd.pkl**: Pathway mask file defining which pathways to analyze\n",
        "- **timm.tar**: Timm library archive (vision transformer dependencies)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note**: Runtime may reset during installation - this is expected and normal.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Install Timm Library\n",
        "\n",
        "Install the timm (PyTorch Image Models) library from the downloaded archive. This library provides the vision transformer backbone used by the PATH model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Load and Visualize Spatial Data\n",
        "\n",
        "Load the spatial transcriptomics data using scanpy and visualize the spatial distribution of Leiden clusters. This helps understand the spatial organization of the tissue.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check the shape of the processed images to verify data loading.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Process Coordinates and Load Mask\n",
        "\n",
        "Process the spatial coordinates from the AnnData object to extract image patches corresponding to each spot. Load the pathway mask that defines which KEGG pathways are included in the analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Initialize PATH Model\n",
        "\n",
        "Create the PATH model with the following parameters:\n",
        "- `kegg_dim`: Number of pathways (determined by mask)\n",
        "- `lora_rank`: LoRA rank for efficient fine-tuning (8)\n",
        "- `num_slides`, `num_samples`, `num_datasets`: Dataset configuration parameters\n",
        "- `classification_mode`: Set to True for pathway prediction\n",
        "\n",
        "Load the pre-trained weights from the downloaded checkpoint.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Create DataLoader\n",
        "\n",
        "Define a simple PyTorch Dataset class to handle the image data and create a DataLoader for batch processing. The batch size is set to 128 for efficient GPU utilization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Install additional dependencies for graph-based clustering (igraph and leidenalg).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Extract Embeddings\n",
        "\n",
        "Extract embeddings from the PATH model for all spots in the dataset. These embeddings capture spatial and molecular features learned by the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Clustering Analysis\n",
        "\n",
        "Perform downstream analysis using the extracted embeddings:\n",
        "\n",
        "1. Add embeddings to the AnnData object\n",
        "2. Compute k-nearest neighbors graph using the embeddings\n",
        "3. Perform Leiden clustering to identify spatial domains\n",
        "4. Visualize clusters on the spatial coordinates\n",
        "\n",
        "The resolution parameter (0.15) controls the granularity of clustering - lower values produce fewer, larger clusters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 11: Pathway Activity Analysis\n",
        "\n",
        "Perform differential pathway activity analysis between clusters:\n",
        "\n",
        "1. **Statistical Testing**: Use Mann-Whitney U test to identify pathways enriched in each cluster compared to the rest\n",
        "2. **Multiple Testing Correction**: Apply Benjamini-Hochberg FDR correction\n",
        "3. **Visualization**: Create a clustered heatmap showing pathway activities across clusters\n",
        "\n",
        "The analysis identifies pathways that are significantly enriched in each spatial domain, providing biological insights into regional tissue function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 12: Predict Pathway Activities\n",
        "\n",
        "Use the model's KEGG head to predict pathway activities from the extracted embeddings. This generates pathway activity scores for each spot, which can be used for downstream biological interpretation.\n",
        "\n",
        "**Note**: Ensure you have CUDA available for GPU acceleration, otherwise the model will run on CPU (slower).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b86eab6b"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "file_id_obj = \"1vICnccokrUrOTcNbNd-FXortyf0QNNGM\"\n",
        "output_path_obj = './data/hd_obj.h5ad' # Replace with your desired output filename and extension\n",
        "file_id_wights = \"1KXvTXc6XnPASSY652lOaF9ZQCvd0cCkk\"\n",
        "output_file_wights = './models/hd_wights.pth'\n",
        "file_id_mask = \"1uAIxdMRwCseJI4Dkr9tXRwcvGYJzn0x5\"\n",
        "output_file_mask = './data/mask_hd.pkl'\n",
        "file_id_timm =\"1L7dbztMHC-ipFrlILLcX8GXGne3mr6fZ\"\n",
        "out_put_timm = \"./timm.tar\"\n",
        "gdown.download(id=file_id_obj, output=output_path_obj, quiet=False)\n",
        "gdown.download(id=file_id_wights, output=output_file_wights, quiet=False)\n",
        "gdown.download(id=file_id_mask, output=output_file_mask, quiet=False)\n",
        "gdown.download(id=file_id_timm, output=out_put_timm, quiet=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIRb0My8LfyV"
      },
      "source": [
        "run time will be reset - its ok!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfMQ9SOSK-H9"
      },
      "outputs": [],
      "source": [
        "!pip install timm.tar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awOACnslJGIP"
      },
      "outputs": [],
      "source": [
        "import scanpy as sc\n",
        "import os\n",
        "os.chdir(\"PATH\")\n",
        "obj = sc.read_h5ad('./data/hd_obj.h5ad')\n",
        "sc.pl.spatial(obj, color=[\"leiden\"], spot_size=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iv6ixJGCJbXG"
      },
      "outputs": [],
      "source": [
        "import scripts.Utils as ut\n",
        "import torch\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "imgs = ut.process_coord_obj_hd(obj)\n",
        "with open('./data/mask_hd.pkl', 'rb') as f:\n",
        "    mask = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRPb7y9UMkGD"
      },
      "outputs": [],
      "source": [
        "import src.PATH as path\n",
        "model = path.create_model(kegg_dim=mask.sum(),lora_rank=8,num_slides=1, num_samples=1, num_datasets=1,classification_mode=True)\n",
        "model.load_model(r\"./models/hd_wights.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRpKcq9EMqce"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "class basic_dataset(Dataset):\n",
        "    def __init__(self, images):\n",
        "        self.images = images\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.images[idx]\n",
        "\n",
        "dataloader = DataLoader(basic_dataset(imgs), batch_size=128, shuffle=False, drop_last=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0zdGlNOVGal"
      },
      "outputs": [],
      "source": [
        "!pip3 install igraph leidenalg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUtIGGWsQlb4"
      },
      "outputs": [],
      "source": [
        "encoded = ut.get_embeddings(model, dataloader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zut3jbW8TrK8"
      },
      "outputs": [],
      "source": [
        "\n",
        "obj.obsm[\"X_embed\"] = encoded\n",
        "sc.pp.neighbors(obj, use_rep=\"X_embed\", n_neighbors=15)\n",
        "sc.tl.leiden(obj, resolution=0.15, key_added=\"embedding_leiden\")\n",
        "\n",
        "# See real number of clusters found\n",
        "num_leiden_clusters = len(obj.obs[\"embedding_leiden\"].unique())\n",
        "print(f\"Leiden clusters found: {num_leiden_clusters}\")\n",
        "\n",
        "# Plot clusters on spatial locations\n",
        "sc.pl.spatial(obj, color=\"embedding_leiden\", spot_size=100, show=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-ok-uRZUeJ0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import mannwhitneyu\n",
        "import seaborn as sns\n",
        "\n",
        "def bh_fdr(pvals: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Benjamini–Hochberg FDR for a 1D array.\"\"\"\n",
        "    pvals = np.asarray(pvals, dtype=float)\n",
        "    n = pvals.size\n",
        "    order = np.argsort(pvals)\n",
        "    ranked = pvals[order]\n",
        "    q = ranked * n / (np.arange(1, n + 1))\n",
        "    # enforce monotonicity\n",
        "    q = np.minimum.accumulate(q[::-1])[::-1]\n",
        "    q = np.clip(q, 0, 1)\n",
        "    out = np.empty_like(q)\n",
        "    out[order] = q\n",
        "    return out\n",
        "\n",
        "def cluster_vs_rest_pathways(\n",
        "    predictions: np.ndarray,\n",
        "    cluster_labels,\n",
        "    pathway_names,\n",
        "    *,\n",
        "    fdr_thresh: float = 0.05,\n",
        "    min_mean_diff: float = 0.0,\n",
        "    alternative: str = \"greater\",   # \"greater\" = enriched in cluster; \"two-sided\" also possible\n",
        "    top_k_fallback: int = 30,\n",
        "):\n",
        "    \"\"\"\n",
        "    Cluster vs rest DE-like pathway testing using Mann–Whitney U per pathway.\n",
        "\n",
        "    Returns:\n",
        "      results_per_cluster: dict cluster -> DataFrame (sorted by qval then effect)\n",
        "      selected_pathways: list of pathway names (union of significant, or fallback top_k)\n",
        "    \"\"\"\n",
        "    X = np.asarray(predictions)\n",
        "    if X.ndim != 2:\n",
        "        raise ValueError(f\"predictions must be 2D (cells x pathways). Got shape {X.shape}\")\n",
        "    n_cells, n_path = X.shape\n",
        "\n",
        "    labels = np.asarray(cluster_labels)\n",
        "    if labels.shape[0] != n_cells:\n",
        "        raise ValueError(f\"cluster_labels length {labels.shape[0]} != n_cells {n_cells}\")\n",
        "\n",
        "    pathway_names = np.asarray(pathway_names)\n",
        "    if pathway_names.shape[0] != n_path:\n",
        "        raise ValueError(f\"pathway_names length {pathway_names.shape[0]} != n_path {n_path}\")\n",
        "\n",
        "    unique_clusters = pd.unique(labels)\n",
        "\n",
        "    results_per_cluster = {}\n",
        "    selected = set()\n",
        "\n",
        "    for c in unique_clusters:\n",
        "        in_mask = (labels == c)\n",
        "        out_mask = ~in_mask\n",
        "\n",
        "        Xin = X[in_mask]\n",
        "        Xout = X[out_mask]\n",
        "\n",
        "        # Effect size: mean difference (cluster - rest)\n",
        "        mean_in = Xin.mean(axis=0)\n",
        "        mean_out = Xout.mean(axis=0)\n",
        "        mean_diff = mean_in - mean_out\n",
        "\n",
        "        # Wilcoxon/MWU p-values per pathway\n",
        "        pvals = np.ones(n_path, dtype=float)\n",
        "        for j in range(n_path):\n",
        "            a = Xin[:, j]\n",
        "            b = Xout[:, j]\n",
        "\n",
        "            # If constant / identical distributions, MWU can be unstable; guard lightly\n",
        "            if np.all(a == a[0]) and np.all(b == b[0]) and a[0] == b[0]:\n",
        "                pvals[j] = 1.0\n",
        "                continue\n",
        "\n",
        "            # mannwhitneyu handles ties with method=\"auto\" (SciPy>=1.7-ish)\n",
        "            pvals[j] = mannwhitneyu(a, b, alternative=alternative, method=\"auto\").pvalue\n",
        "\n",
        "        qvals = bh_fdr(pvals)\n",
        "\n",
        "        df = pd.DataFrame({\n",
        "            \"pathway\": pathway_names,\n",
        "            \"mean_in\": mean_in,\n",
        "            \"mean_out\": mean_out,\n",
        "            \"mean_diff\": mean_diff,\n",
        "            \"pval\": pvals,\n",
        "            \"qval\": qvals,\n",
        "        })\n",
        "\n",
        "        # Filter + sort\n",
        "        df_sig = df[(df[\"qval\"] <= fdr_thresh) & (df[\"mean_diff\"] >= min_mean_diff)].copy()\n",
        "        df_sig.sort_values([\"qval\", \"mean_diff\"], ascending=[True, False], inplace=True)\n",
        "\n",
        "        # If nothing passes, keep a fallback top-k by mean_diff (still useful for plots)\n",
        "        if df_sig.empty:\n",
        "            df_fallback = df.sort_values(\"mean_diff\", ascending=False).head(top_k_fallback)\n",
        "            chosen = df_fallback[\"pathway\"].tolist()\n",
        "        else:\n",
        "            chosen = df_sig[\"pathway\"].tolist()\n",
        "\n",
        "        selected.update(chosen)\n",
        "        results_per_cluster[c] = (df_sig if not df_sig.empty else df.sort_values(\"mean_diff\", ascending=False))\n",
        "\n",
        "    selected_pathways = list(selected)\n",
        "    return results_per_cluster, selected_pathways\n",
        "cluster_labels = recon_obj.obs[\"embedding_leiden\"].values\n",
        "pathway_names = np.asarray(S.columns)[mask]  # keep your existing naming\n",
        "\n",
        "results_per_cluster, selected_pathways = cluster_vs_rest_pathways(\n",
        "    predictions=predictions,\n",
        "    cluster_labels=cluster_labels,\n",
        "    pathway_names=pathway_names,\n",
        "    fdr_thresh=0.05,\n",
        "    min_mean_diff=0.0,\n",
        "    alternative=\"greater\",\n",
        "    top_k_fallback=15,\n",
        ")\n",
        "\n",
        "# Build your cluster x pathway heatmap matrix using selected_pathways\n",
        "avg_pathways_df = pd.DataFrame(\n",
        "    [predictions[cluster_labels == c].mean(axis=0) for c in pd.unique(cluster_labels)],\n",
        "    index=[str(c) for c in pd.unique(cluster_labels)],\n",
        "    columns=pathway_names\n",
        ")\n",
        "\n",
        "heatmap_df = avg_pathways_df[selected_pathways]\n",
        "heatmap_df_z = heatmap_df.sub(heatmap_df.mean(axis=1), axis=0).div(heatmap_df.std(axis=1).replace(0, 1), axis=0)\n",
        "ax = sns.clustermap(heatmap_df_z.T, cmap=\"coolwarm\", figsize=(10, 15), vmin=-1.5, vmax=1.5)\n",
        "plt.savefig(f\"./figs/pathway_heatmap_hd.pdf\", format='pdf', bbox_inches='tight')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEZtaG-JTkBg"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "prediction = model.kegg_head(torch.tensor(encoded).to(device))\n",
        "prediction.shape"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
